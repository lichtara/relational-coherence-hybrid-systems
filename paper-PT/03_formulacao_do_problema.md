## 03. Formulação do Problema

03. Problema de Pesquisa

A rápida integração da inteligência artificial em sistemas institucionais, organizacionais e societais produziu uma lacuna persistente e crescente entre desempenho técnico e governabilidade sistêmica. Sistemas híbridos humano–IA frequentemente operam com altos níveis de eficiência — gerando resultados precisos, otimizando processos e escalando decisões — enquanto simultaneamente se tornam mais difíceis de compreender, contestar e responsabilizar. O problema central não é falha técnica, mas uma erosão estrutural da clareza relacional: a diminuição da rastreabilidade da agência, da justificativa e da responsabilidade através das camadas humanas, institucionais e algorítmicas.

Este artigo parte de uma distinção rigorosa: desempenho não é equivalente a governabilidade. Um sistema pode funcionar, coordenar e até otimizar resultados sem sustentar as condições estruturais sob as quais responsabilidade, accountability e sentido compartilhado permanecem inteligíveis. Quando essas condições se deterioram, o que persiste é controle operacional ou conformidade procedimental — não governabilidade em sentido pleno.

As abordagens existentes de governança em IA têm dificuldade em tratar essa distinção em nível estrutural. Os frameworks dominantes tendem a concentrar-se em uma de três dimensões: interação, otimização ou regulação.

Modelos centrados na interação analisam as relações humano–IA no nível de interfaces, pontos decisórios discretos ou ciclos de feedback. Embora analiticamente precisos, geralmente pressupõem um ambiente relacional estável no qual as interações ocorrem. Raramente examinam como esse próprio ambiente pode gradualmente perder coerência. Como resultado, mostram-se limitados para explicar fenômenos sistêmicos como erosão de legitimidade, normalização da opacidade ou colapso da produção coletiva de sentido — mesmo quando interações individuais permanecem frequentes e tecnicamente funcionais.

Abordagens orientadas à otimização, prevalentes na pesquisa em alinhamento, na teoria do controle e na governança focada em desempenho, deslocam a atenção das trocas discretas para os resultados sistêmicos. A coerência é enquadrada principalmente em termos de consistência, eficiência ou convergência em direção a objetivos previamente definidos. Contudo, otimização sem limites estruturais explícitos corre o risco de produzir formas de coerência coercitiva: alinhamento no nível dos outputs que oculta fragmentação normativa, difusão de responsabilidade ou instabilidade temporal. Sistemas podem parecer alinhados enquanto se tornam estruturalmente frágeis, suprimindo julgamento plural e mascarando incoerências mais profundas.

Modelos regulatórios e baseados em compliance, por sua vez, enfatizam fronteiras, restrições e mecanismos formais de responsabilização. São indispensáveis. No entanto, quando limites são impostos sem uma compreensão integrada do campo relacional e dos processos que sustentam a coerência em seu interior, correm o risco de se tornar simbólicos, seletivamente aplicados ou até desestabilizadores. Fronteiras que não correspondem às dinâmicas relacionais subjacentes podem falhar em sustentar accountability e até acelerar processos de deriva sistêmica.

Através dessas perspectivas, observa-se uma fragmentação comum. Modelos de interação enfatizam relações, mas negligenciam limites estruturais. Modelos de otimização enfatizam coerência, mas abstraem das condições de campo e da responsabilidade. Modelos regulatórios enfatizam limites, mas frequentemente os desconectam dos processos relacionais e de coerência. Cada abordagem captura uma dimensão necessária; nenhuma oferece um relato estrutural integrado da governabilidade enquanto tal.

O problema central abordado neste artigo, portanto, não é a ausência de mecanismos de governança, mas a ausência de um modelo estrutural capaz de explicar como a governabilidade se torna possível — e como se deteriora — em sistemas híbridos humano–IA caracterizados por agência distribuída, alta densidade relacional e acoplamento institucional.

A governabilidade, como aqui defendido, pressupõe uma estrutura relacional mínima e irredutível. Na ausência dessa estrutura, sistemas híbridos podem coordenar ações, otimizar desempenho e cumprir regras, mas não permanecem governáveis em sentido substantivo. A tese avançada não é apenas analítica, mas estrutural: a governabilidade em sistemas híbridos humano–IA depende da interação sustentada de três elementos irredutíveis — Campo, Coerência e Limite.

Campo designa o ambiente relacional no qual agentes, instituições e modelos operam e adquirem orientação interpretativa.
Coerência designa os processos pelos quais sentido, justificativa e continuidade temporal são estabilizados nesse ambiente.
Limite designa as fronteiras que preservam accountability, restringem poder e mantêm as condições de contestabilidade.

Esses elementos são mutuamente constitutivos. Remova o Campo, e a agência torna-se ininteligível. Remova a Coerência, e o sentido compartilhado se fragmenta apesar da continuidade operacional. Remova o Limite, e a responsabilidade se dissolve mesmo sob aparente alinhamento. Em cada caso, o que colapsa não é a eficiência, mas a própria governabilidade.

A fragmentação das abordagens existentes obscurece sinais precoces de deriva estrutural. Sistemas híbridos podem satisfazer métricas de desempenho e requisitos regulatórios enquanto perdem progressivamente legibilidade relacional e difundem responsabilidade. As falhas de governança tendem, assim, a ser detectadas apenas após a materialização do dano, quando a intervenção se torna reativa e custosa.

Ao reformular a governança de IA como um problema estrutural de coerência relacional, este artigo desloca o foco analítico do controle de comportamento e da otimização de outputs para as condições que tornam agência, responsabilidade e sentido sustentáveis ao longo do tempo. O modelo triádico desenvolvido nas seções seguintes não propõe mais um framework entre outros; ele articula as condições estruturais mínimas sob as quais a governabilidade permanece possível em sistemas híbridos humano–IA.
