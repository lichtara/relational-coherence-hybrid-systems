## 03. Formulação do Problema

A rápida integração da inteligência artificial em sistemas institucionais, organizacionais e sociais revelou uma lacuna crescente entre o desempenho técnico dos sistemas e sua governabilidade sistêmica. Muitos sistemas híbridos humano–IA continuam a funcionar de forma eficiente — produzindo outputs precisos, otimizando processos e escalando decisões — enquanto, simultaneamente, se tornam mais difíceis de compreender, contestar e responsabilizar. O que emerge não é, primariamente, uma falha técnica, mas uma perda de clareza relacional: uma incerteza sobre quem está agindo, com base em quais fundamentos e sob quais condições a responsabilidade pode ser exercida de maneira significativa entre camadas humanas, institucionais e algorítmicas.

As abordagens atuais de governança da IA enfrentam dificuldades para lidar com essa lacuna em nível estrutural. Os frameworks dominantes tendem a enfatizar interações isoladas entre agentes, a otimização e o alinhamento dos outputs dos sistemas, ou a imposição de fronteiras formais por meio de regras, auditorias e mecanismos de conformidade. Embora cada uma dessas abordagens capte uma dimensão relevante dos sistemas híbridos humano–IA, elas geralmente operam de forma isolada e carecem de um relato integrado de como a governabilidade é sustentada — ou gradualmente erodida — sob condições de alta densidade relacional, automação crescente e forte acoplamento institucional.

Modelos centrados na interação tendem a analisar as relações humano–IA no nível de interfaces, pontos discretos de decisão ou ciclos de feedback. Embora sejam analiticamente valiosas, essas abordagens frequentemente pressupõem a existência de um ambiente relacional estável no qual as interações ocorrem. Como resultado, têm dificuldade em dar conta da degradação gradual do sentido compartilhado, da confiança e da orientação interpretativa — processos que podem se desenrolar mesmo quando as interações permanecem frequentes, eficientes e tecnicamente funcionais. Essa limitação dificulta a explicação de fenômenos sistêmicos como a erosão da legitimidade, a normalização de processos decisórios opacos ou as rupturas na construção coletiva de sentido no interior de sistemas híbridos.

Modelos orientados à otimização, comuns em alinhamento de IA, teoria do controle e governança focada em desempenho, tendem a definir coerência primariamente em termos de consistência, eficiência ou convergência em direção a objetivos previamente estabelecidos. Na ausência de um relato estrutural explícito sobre fronteiras ou limites, tais abordagens correm o risco de produzir formas de coerência coercitiva que suprimem o julgamento plural, obscurecem a responsabilidade e geram comportamentos sistêmicos frágeis. Um alinhamento aparente no nível dos outputs pode ocultar incoerências mais profundas em dimensões normativas, institucionais ou temporais, deixando o sistema vulnerável apesar do desempenho superficial.


Abordagens regulatórias e baseadas em conformidade, por sua vez, enfatizam fronteiras, restrições e mecanismos formais de responsabilização. Embora indispensáveis, essas abordagens frequentemente operam sem uma compreensão relacional dos sistemas que buscam governar. Limites impostos sem sensibilidade às dinâmicas de campo e aos processos de coerência tendem a se tornar simbólicos, aplicados de forma seletiva ou até desestabilizadores, falhando em sustentar uma responsabilização significativa ou capacidade adaptativa.

O problema central abordado neste artigo não é, portanto, a ausência de mecanismos de governança, mas a falta de um modelo estrutural integrado capaz de explicar como a governabilidade emerge e se deteriora em sistemas híbridos humano–IA. Os frameworks existentes tendem a tratar condições de campo, processos de coerência e limites como preocupações separáveis, em vez de elementos mutuamente constitutivos de uma única estrutura relacional.

Essa fragmentação obscurece os sinais iniciais de deriva sistêmica. Sistemas híbridos podem continuar a cumprir métricas de desempenho e requisitos de conformidade enquanto, progressivamente, perdem legibilidade relacional, difundem a responsabilidade e corroem a confiança institucional. As falhas de governança, assim, tendem a ser detectadas apenas após a ocorrência de danos, quando intervenções corretivas se tornam custosas, contestadas ou ineficazes.

Este artigo aborda essa lacuna ao formular o problema da governança da IA como um problema estrutural de coerência relacional. Argumenta-se que a governabilidade em sistemas híbridos humano–IA depende de uma estrutura triádica mínima e irredutível composta por Campo, Coerência e Limite. Sem um relato explícito de como esses elementos interagem, os esforços de governança permanecem reativos, fragmentados e vulneráveis a falhas em cascata.

Ao articular esse problema, o artigo desloca o foco da governança da IA do controle de comportamentos ou da otimização de outputs para a sustentação das condições estruturais sob as quais agência, responsabilização e sentido permanecem inteligíveis ao longo do tempo. Esse reenquadramento estabelece a base para o modelo triádico desenvolvido nas seções subsequentes e para as implicações de governança adaptativa dele derivadas.
