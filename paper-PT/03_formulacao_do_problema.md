## 03. Formulação do Problema

A rápida integração da inteligência artificial em sistemas institucionais, organizacionais e sociais revelou uma lacuna crescente entre o desempenho técnico dos sistemas e sua governabilidade sistêmica. Muitos sistemas híbridos humano–IA continuam a funcionar de forma eficiente — produzindo outputs precisos, otimizando processos e escalando decisões — enquanto, simultaneamente, se tornam mais difíceis de compreender, contestar e responsabilizar. O que emerge não é, primariamente, uma falha técnica, mas uma perda de clareza relacional: uma incerteza sobre quem está agindo, com base em quais fundamentos e sob quais condições a responsabilidade pode ser exercida de maneira significativa entre camadas humanas, institucionais e algorítmicas.

As abordagens atuais de governança da IA enfrentam dificuldades para lidar com essa lacuna em nível estrutural. Os frameworks dominantes tendem a enfatizar interações isoladas entre agentes, a otimização e o alinhamento dos outputs dos sistemas, ou a imposição de fronteiras formais por meio de regras, auditorias e mecanismos de conformidade. Embora cada uma dessas abordagens capte uma dimensão relevante dos sistemas híbridos humano–IA, elas geralmente operam de forma isolada e carecem de um relato integrado de como a governabilidade é sustentada — ou gradualmente erodida — sob condições de alta densidade relacional, automação crescente e forte acoplamento institucional.

Modelos centrados na interação geralmente analisam as relações humano–IA no nível de interfaces, pontos de decisão ou ciclos de feedback. Embora valiosas, essas abordagens frequentemente pressupõem um ambiente relacional estável e, assim, falham em explicar a degradação do sentido compartilhado, da confiança e da orientação interpretativa que ocorre mesmo quando as interações permanecem frequentes e tecnicamente funcionais. Como resultado, têm dificuldade em explicar fenômenos sistêmicos como a erosão da legitimidade, a normalização de processos decisórios opacos ou as rupturas na construção coletiva de sentido.

Modelos orientados à otimização, predominantes nos campos de alinhamento de IA, teoria do controle e governança baseada em desempenho, tendem a enquadrar a coerência primariamente como consistência, eficiência ou convergência em direção a objetivos previamente definidos. Na ausência de um relato estrutural explícito sobre limites, tais modelos correm o risco de produzir formas de coerência coercitiva que suprimem o julgamento plural, obscurecem a responsabilidade e geram sistemas frágeis. Um alinhamento aparente no nível dos outputs pode mascarar incoerências mais profundas em dimensões normativas, institucionais ou temporais.

Abordagens regulatórias e baseadas em conformidade, por sua vez, enfatizam fronteiras, restrições e mecanismos formais de responsabilização. Embora indispensáveis, essas abordagens frequentemente operam sem uma compreensão relacional dos sistemas que buscam governar. Limites impostos sem sensibilidade às dinâmicas de campo e aos processos de coerência tendem a se tornar simbólicos, aplicados de forma seletiva ou até desestabilizadores, falhando em sustentar uma responsabilização significativa ou capacidade adaptativa.

O problema central abordado neste artigo não é, portanto, a ausência de mecanismos de governança, mas a falta de um modelo estrutural integrado capaz de explicar como a governabilidade emerge e se deteriora em sistemas híbridos humano–IA. Os frameworks existentes tendem a tratar condições de campo, processos de coerência e limites como preocupações separáveis, em vez de elementos mutuamente constitutivos de uma única estrutura relacional.

Essa fragmentação obscurece os sinais iniciais de deriva sistêmica. Sistemas híbridos podem continuar a cumprir métricas de desempenho e requisitos de conformidade enquanto, progressivamente, perdem legibilidade relacional, difundem a responsabilidade e corroem a confiança institucional. As falhas de governança, assim, tendem a ser detectadas apenas após a ocorrência de danos, quando intervenções corretivas se tornam custosas, contestadas ou ineficazes.

Este artigo aborda essa lacuna ao formular o problema da governança da IA como um problema estrutural de coerência relacional. Argumenta-se que a governabilidade em sistemas híbridos humano–IA depende de uma estrutura triádica mínima e irredutível composta por Campo, Coerência e Limite. Sem um relato explícito de como esses elementos interagem, os esforços de governança permanecem reativos, fragmentados e vulneráveis a falhas em cascata.

Ao articular esse problema, o artigo desloca o foco da governança da IA do controle de comportamentos ou da otimização de outputs para a sustentação das condições estruturais sob as quais agência, responsabilização e sentido permanecem inteligíveis ao longo do tempo. Esse reenquadramento estabelece a base para o modelo triádico desenvolvido nas seções subsequentes e para as implicações de governança adaptativa dele derivadas.
