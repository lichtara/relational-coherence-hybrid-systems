## 03. Formulação do Problema

A rápida integração da inteligência artificial em sistemas institucionais, organizacionais e sociais revelou uma lacuna crescente entre o desempenho técnico dos sistemas e sua governabilidade sistêmica. Muitos sistemas híbridos humano–IA continuam a funcionar de forma eficiente — produzindo outputs precisos, otimizando processos e escalando decisões — enquanto, simultaneamente, se tornam mais difíceis de compreender, contestar e responsabilizar. O que emerge não é, primariamente, uma falha técnica, mas uma perda de clareza relacional: uma incerteza sobre quem está agindo, com base em quais fundamentos e sob quais condições a responsabilidade pode ser exercida de maneira significativa entre camadas humanas, institucionais e algorítmicas.

As abordagens atuais de governança da IA enfrentam dificuldades para lidar com essa lacuna em nível estrutural. Os frameworks dominantes tendem a enfatizar interações isoladas entre agentes, a otimização e o alinhamento dos outputs dos sistemas, ou a imposição de fronteiras formais por meio de regras, auditorias e mecanismos de conformidade. Embora cada uma dessas abordagens capte uma dimensão relevante dos sistemas híbridos humano–IA, elas geralmente operam de forma isolada e carecem de um relato integrado de como a governabilidade é sustentada — ou gradualmente erodida — sob condições de alta densidade relacional, automação crescente e forte acoplamento institucional.

Modelos centrados na interação geralmente analisam as relações humano–IA no nível de interfaces, pontos discretos de decisão ou ciclos de feedback. Embora analiticamente valiosas, essas abordagens frequentemente pressupõem a existência de um ambiente relacional estável no qual as interações ocorrem. Como resultado, têm dificuldade em dar conta da degradação gradual do sentido compartilhado, da confiança e da orientação interpretativa, processos que podem se desenrolar mesmo quando as interações permanecem frequentes, eficientes e tecnicamente funcionais. Fenômenos sistêmicos como a erosão da legitimidade, a normalização de processos decisórios opacos ou as rupturas na construção coletiva de sentido tornam-se, assim, difíceis de explicar a partir de frameworks puramente interacionais.

Modelos orientados à otimização, comuns em alinhamento de IA, teoria do controle e governança focada em desempenho, procuram responder a algumas dessas limitações ao enfatizar consistência, eficiência e convergência em direção a objetivos previamente definidos. No entanto, quando a coerência é definida primariamente em termos de alinhamento de outputs, e sem um relato estrutural explícito sobre fronteiras ou limites, tais abordagens correm o risco de produzir formas de coerência coercitiva. Essas configurações tendem a suprimir o julgamento plural, obscurecer a responsabilidade e gerar comportamentos sistêmicos frágeis. Um alinhamento aparente no nível do desempenho pode, assim, ocultar incoerências mais profundas em dimensões normativas, institucionais ou temporais, deixando o sistema vulnerável apesar do sucesso técnico.

Abordagens regulatórias e baseadas em conformidade, por sua vez, colocam em primeiro plano fronteiras, restrições e mecanismos formais de responsabilização como instrumentos centrais de governança. Embora indispensáveis, essas abordagens frequentemente operam sem uma compreensão relacional dos sistemas que buscam governar. Limites impostos sem sensibilidade às dinâmicas de campo e aos processos de coerência podem se tornar simbólicos, aplicados de forma seletiva ou até ativamente desestabilizadores. Nesses casos, as fronteiras deixam de sustentar uma responsabilização significativa ou capacidade adaptativa e podem, inadvertidamente, minar as próprias condições relacionais necessárias para uma governança eficaz.

O problema central abordado neste artigo não é, portanto, a ausência de mecanismos de governança, mas a falta de um modelo estrutural integrado capaz de explicar como a governabilidade emerge e se deteriora em sistemas híbridos humano–IA. Os frameworks existentes tendem a tratar condições de campo, processos de coerência e limites como preocupações separáveis, em vez de elementos mutuamente constitutivos de uma única estrutura relacional.

Essa fragmentação obscurece os sinais iniciais de deriva sistêmica. Sistemas híbridos podem continuar a cumprir métricas de desempenho e requisitos de conformidade enquanto, progressivamente, perdem legibilidade relacional, difundem a responsabilidade e corroem a confiança institucional. As falhas de governança, assim, tendem a ser detectadas apenas após a ocorrência de danos, quando intervenções corretivas se tornam custosas, contestadas ou ineficazes.

Este artigo aborda essa lacuna ao formular o problema da governança da IA como um problema estrutural de coerência relacional. Argumenta-se que a governabilidade em sistemas híbridos humano–IA depende de uma estrutura triádica mínima e irredutível composta por Campo, Coerência e Limite. Sem um relato explícito de como esses elementos interagem, os esforços de governança permanecem reativos, fragmentados e vulneráveis a falhas em cascata.

Ao articular esse problema, o artigo desloca o foco da governança da IA do controle de comportamentos ou da otimização de outputs para a sustentação das condições estruturais sob as quais agência, responsabilização e sentido permanecem inteligíveis ao longo do tempo. Esse reenquadramento estabelece a base para o modelo triádico desenvolvido nas seções subsequentes e para as implicações de governança adaptativa dele derivadas.
